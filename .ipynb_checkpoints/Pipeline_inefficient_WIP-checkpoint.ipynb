{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "#import scipy as sp\n",
    "#import math\n",
    "from time import sleep\n",
    "pd.set_option('display.max_colwidth',1000)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Mon Sep 10 14:12:11 2018\n",
    "\n",
    "@author: rajat13440\n",
    "\"\"\"\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen, Request\n",
    "from socket import timeout\n",
    "import http\n",
    "import gc\n",
    "import nltk\n",
    "import unicodedata\n",
    "import re\n",
    "\n",
    "summaryDict={}\n",
    "\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "\n",
    "try:\n",
    "    nltk.data.find('corpora/words')\n",
    "except LookupError:\n",
    "     nltk.download('words')\n",
    "\n",
    "#try:\n",
    "#    nltk.data.find('sentiment/vader_lexicon')\n",
    "#except LookupError:\n",
    "#    nltk.download('vader_lexicon')\n",
    "\n",
    "try:\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')\n",
    "\n",
    "#try:\n",
    "#    nltk.data.find('tokenizers/punkt')\n",
    "#except LookupError:\n",
    "#    nltk.download('wordnet')\n",
    "\n",
    "#class ProcessData:\n",
    "#    \n",
    "#    def __init__(self,x):\n",
    "#        \n",
    "#        self.GetSentenceTokens(x)\n",
    "#        self.Normalize(x)\n",
    "\n",
    "class Extract:\n",
    "    def GetText(self,link):\n",
    "        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2228.0 Safari/537.3',\n",
    "        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
    "        'Accept-Charset': 'ISO-8859-1,utf-8;q=0.7,*;q=0.3',\n",
    "        'Accept-Encoding': 'none',\n",
    "        'Accept-Language': 'en-US,en;q=0.8',\n",
    "        'Connection': 'keep-alive'}\n",
    "        try:\n",
    "            req = Request(link, headers=headers)\n",
    "            with urlopen(req) as response:\n",
    "                page = response.read()\n",
    "                soup=BeautifulSoup(page, 'html.parser')\n",
    "                ptag=soup.find_all('p')\n",
    "                ''' Analysis\n",
    "                 #if len(ptag)<5:\n",
    "                  #print(\"VERY LOW P TAGS, STUDY THIS - %s - %d\" %(link, len(ptag)))\n",
    "                  #texta=link + \"  **VERY LOW P TAGS, STUDY THIS \" '''\n",
    "            text=[]\n",
    "            for tag in ptag:\n",
    "                text.append(tag.get_text())\n",
    "                texta=str(' '.join(text)).replace('\\n',' ')\n",
    "\n",
    "            #print(\"processed {} {} \\n\".format(link,b))\n",
    "            #b+=1\n",
    "        except http.client.IncompleteRead:\n",
    "            #print(\"INCOMPLETE READ - {}\".format(link))\n",
    "            texta=\"ERROR: \"+link + \"  **INCOMPLETE READ\"\n",
    "        except WindowsError as e:\n",
    "            #print(\"WINDOWS ERROR {} {}\".format(e,link))\n",
    "            texta=\"ERROR: \"+link + \"  **WINDOWS ERROR {}\".format(e)\n",
    "        except BaseException as error:\n",
    "            #print(\"SOME OTHER ERROR {}\".format(link))\n",
    "            texta=\"ERROR: \"+link + \"  **SOME OTHER ERROR  {}\".format(error)\n",
    "        except timeout:\n",
    "            #print(\"LINK {} TIMED OUT \\n\".format(link))\n",
    "            #timedout.append(link)\n",
    "            texta=\"ERROR: \"+link + \"  **TIMED OUT\"\n",
    "        del text,headers\n",
    "        gc.collect    \n",
    "        return texta\n",
    "    \n",
    "    def _Frequency(self,wordslist):\n",
    "        trimmeddictlist=[]\n",
    "        freq={}\n",
    "        for word in wordslist:\n",
    "            freq[word]=wordslist.count(word)\n",
    "            # gets only those words which are frequent than mean word frequency of that text\n",
    "            trimmedfreq={k:v for (k,v) in freq.items() if v >(float(sum(freq.values())) / len(freq))}    \n",
    "            trimmeddictlist.append(trimmedfreq)\n",
    "        return trimmedfreq\n",
    "\n",
    "    def Summary(self,article):\n",
    "        #article=GetText(url)\n",
    "        percent=(20/100)\n",
    "        sentokens=self.GetSentenceTokens(article)\n",
    "        tokens=self.Normalize(article)\n",
    "        freqlist=self._Frequency(tokens)\n",
    "        #sentencerank=[]\n",
    "        sentf={}\n",
    "        for sentence in sentokens:\n",
    "            msentence=sentence.lower().split(' ')\n",
    "            a=0\n",
    "            for word in freqlist:\n",
    "                if word.lower() in msentence:\n",
    "                    a+=1\n",
    "                    sentf[sentence]=a\n",
    "        slength=percent*len(sentokens)\n",
    "        sorteddict=sorted(sentf, key=sentf.get, reverse=True)[:int(slength)]\n",
    "        #sentencerank.append(sorteddict)\n",
    "    \n",
    "        summary=str(' '.join(sorteddict))\n",
    "        return summary\n",
    "    \n",
    "\n",
    "        \n",
    "    def GetSentenceTokens(self,x):\n",
    "        y=nltk.sent_tokenize(x)\n",
    "        return y    \n",
    "        \n",
    "    def _remove_stopwords(self,words):\n",
    "        \"\"\"Remove stop words from list of tokenized words\"\"\"\n",
    "        new_words = []\n",
    "        for word in words:\n",
    "            if word not in nltk.corpus.stopwords.words('english'):\n",
    "                new_words.append(word)\n",
    "        return new_words\n",
    "    \n",
    "    def _lower(self,x):\n",
    "        norm=[word.lower() for word in x]\n",
    "        return norm\n",
    "\n",
    "    def _remove_non_ascii(self,words):\n",
    "        \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n",
    "        new_words = []\n",
    "        for word in words:\n",
    "            new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "            new_words.append(new_word)\n",
    "        return new_words\n",
    "\n",
    "    def _remove_punctuation(self,words):\n",
    "        \"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
    "        new_words = []\n",
    "        for word in words:\n",
    "            new_word = re.sub(r'[^\\w\\s]', '', word)\n",
    "            if new_word != '':\n",
    "                new_words.append(new_word)\n",
    "        return new_words\n",
    "\n",
    "    def Normalize(self,x):\n",
    "        x1=nltk.word_tokenize(x)\n",
    "        x2=self._remove_stopwords(x1)\n",
    "        x3=self._remove_punctuation(x2)\n",
    "        x4=self._remove_non_ascii(x3)\n",
    "        x5=self._lower(x4)\n",
    "        return x5\n",
    "    \n",
    "    def Summarize(self,url):\n",
    "        import pandas as pd\n",
    "        import datetime\n",
    "        #import csv\n",
    "        import numpy as np\n",
    "        import time\n",
    "        starttime=time.time()\n",
    "        exclude=['twitter','glassdoor','youtube','wordpress','facebook','wikipedia','play.google','nasdaq']\n",
    "        startTime_Main = datetime.datetime.now().strftime('%H:%M:%S')\n",
    "        summarylist=[]\n",
    "        summary1=\"\"\n",
    "        #print(\"---START TIME %s ---\" % startTime_Main)\n",
    "        if any (s in url for s in exclude):\n",
    "            #print(\"Not allowed -\",url)\n",
    "            summary1=\"Not Allowed {}\".format(url)\n",
    "        else:\n",
    "            try:\n",
    "                article=self.GetText(url)\n",
    "            except BaseException as e:\n",
    "                article=\"ERROR {} {}\".format(e,url)\n",
    "            \n",
    "            if not article.startswith(\"ERROR\"):\n",
    "                print(\"fetching summary for - \",url)\n",
    "                summary1=self.Summary(article)\n",
    "            else:\n",
    "                summary1=article\n",
    "        \n",
    "        summaryDict[url]=summary1\n",
    "        #print(\"{:.2f}\".format((time.time()-starttime)))\n",
    "        return summary1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Mon Sep 10 14:12:11 2018\n",
    "\n",
    "@author: rajat13440\n",
    "\"\"\"\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen, Request\n",
    "from socket import timeout\n",
    "import http\n",
    "import gc\n",
    "import nltk\n",
    "import unicodedata\n",
    "import re\n",
    "from sklearn.base import TransformerMixin,BaseEstimator\n",
    "summaryDict={}\n",
    "\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "\n",
    "try:\n",
    "    nltk.data.find('corpora/words')\n",
    "except LookupError:\n",
    "     nltk.download('words')\n",
    "\n",
    "#try:\n",
    "#    nltk.data.find('sentiment/vader_lexicon')\n",
    "#except LookupError:\n",
    "#    nltk.download('vader_lexicon')\n",
    "\n",
    "try:\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')\n",
    "\n",
    "#try:\n",
    "#    nltk.data.find('tokenizers/punkt')\n",
    "#except LookupError:\n",
    "#    nltk.download('wordnet')\n",
    "\n",
    "#class ProcessData:\n",
    "#    \n",
    "#    def __init__(self,x):\n",
    "#        \n",
    "#        self.GetSentenceTokens(x)\n",
    "#        self.Normalize(x)\n",
    "\n",
    "class ExtractNew(BaseEstimator,TransformerMixin):\n",
    "    def transform(self,links,**kwargs):\n",
    "        textdict={}\n",
    "        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2228.0 Safari/537.3',\n",
    "        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
    "        'Accept-Charset': 'ISO-8859-1,utf-8;q=0.7,*;q=0.3',\n",
    "        'Accept-Encoding': 'none',\n",
    "        'Accept-Language': 'en-US,en;q=0.8',\n",
    "        'Connection': 'keep-alive'}\n",
    "        for link in links:\n",
    "            text=[]\n",
    "            try:\n",
    "                req = Request(link, headers=headers)\n",
    "                with urlopen(req) as response:\n",
    "                    page = response.read()\n",
    "                    soup=BeautifulSoup(page, 'html.parser')\n",
    "                    ptag=soup.find_all('p')\n",
    "                    ''' Analysis\n",
    "                     #if len(ptag)<5:\n",
    "                      #print(\"VERY LOW P TAGS, STUDY THIS - %s - %d\" %(link, len(ptag)))\n",
    "                      #texta=link + \"  **VERY LOW P TAGS, STUDY THIS \" '''\n",
    "                \n",
    "                for tag in ptag:\n",
    "                    text.append(tag.get_text())\n",
    "                    texta=str(' '.join(text)).replace('\\n',' ')\n",
    "                \n",
    "                #print(\"processed {} {} \\n\".format(link,b))\n",
    "                #b+=1\n",
    "            except http.client.IncompleteRead:\n",
    "                #print(\"INCOMPLETE READ - {}\".format(link))\n",
    "                texta=\"ERROR: \"+link + \"  **INCOMPLETE READ\"\n",
    "            except WindowsError as e:\n",
    "                #print(\"WINDOWS ERROR {} {}\".format(e,link))\n",
    "                texta=\"ERROR: \"+link + \"  **WINDOWS ERROR {}\".format(e)\n",
    "            except BaseException as error:\n",
    "                #print(\"SOME OTHER ERROR {}\".format(link))\n",
    "                texta=\"ERROR: \"+link + \"  **SOME OTHER ERROR  {}\".format(error)\n",
    "            except timeout:\n",
    "                #print(\"LINK {} TIMED OUT \\n\".format(link))\n",
    "                #timedout.append(link)\n",
    "                texta=\"ERROR: \"+link + \"  **TIMED OUT\"\n",
    "            textdict[link]=texta\n",
    "            #del text,headers\n",
    "            gc.collect    \n",
    "        return textdict\n",
    "    \n",
    "    def fit(self,X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def _Frequency(self,wordslist):\n",
    "        trimmeddictlist=[]\n",
    "        freq={}\n",
    "        for word in wordslist:\n",
    "            freq[word]=wordslist.count(word)\n",
    "            # gets only those words which are frequent than mean word frequency of that text\n",
    "            trimmedfreq={k:v for (k,v) in freq.items() if v >(float(sum(freq.values())) / len(freq))}    \n",
    "            trimmeddictlist.append(trimmedfreq)\n",
    "        return trimmedfreq\n",
    "\n",
    "    def Summary(self,article):\n",
    "        #article=GetText(url)\n",
    "        percent=(20/100)\n",
    "        sentokens=self.GetSentenceTokens(article)\n",
    "        tokens=self.Normalize(article)\n",
    "        freqlist=self._Frequency(tokens)\n",
    "        #sentencerank=[]\n",
    "        sentf={}\n",
    "        for sentence in sentokens:\n",
    "            msentence=sentence.lower().split(' ')\n",
    "            a=0\n",
    "            for word in freqlist:\n",
    "                if word.lower() in msentence:\n",
    "                    a+=1\n",
    "                    sentf[sentence]=a\n",
    "        slength=percent*len(sentokens)\n",
    "        sorteddict=sorted(sentf, key=sentf.get, reverse=True)[:int(slength)]\n",
    "        #sentencerank.append(sorteddict)\n",
    "    \n",
    "        summary=str(' '.join(sorteddict))\n",
    "        return summary\n",
    "    \n",
    "\n",
    "        \n",
    "    def GetSentenceTokens(self,x):\n",
    "        y=nltk.sent_tokenize(x)\n",
    "        return y    \n",
    "        \n",
    "    def _remove_stopwords(self,words):\n",
    "        \"\"\"Remove stop words from list of tokenized words\"\"\"\n",
    "        new_words = []\n",
    "        for word in words:\n",
    "            if word not in nltk.corpus.stopwords.words('english'):\n",
    "                new_words.append(word)\n",
    "        return new_words\n",
    "    \n",
    "    def _lower(self,x):\n",
    "        norm=[word.lower() for word in x]\n",
    "        return norm\n",
    "\n",
    "    def _remove_non_ascii(self,words):\n",
    "        \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n",
    "        new_words = []\n",
    "        for word in words:\n",
    "            new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "            new_words.append(new_word)\n",
    "        return new_words\n",
    "\n",
    "    def _remove_punctuation(self,words):\n",
    "        \"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
    "        new_words = []\n",
    "        for word in words:\n",
    "            new_word = re.sub(r'[^\\w\\s]', '', word)\n",
    "            if new_word != '':\n",
    "                new_words.append(new_word)\n",
    "        return new_words\n",
    "\n",
    "    def Normalize(self,x):\n",
    "        x1=nltk.word_tokenize(x)\n",
    "        x2=self._remove_stopwords(x1)\n",
    "        x3=self._remove_punctuation(x2)\n",
    "        x4=self._remove_non_ascii(x3)\n",
    "        x5=self._lower(x4)\n",
    "        return x5\n",
    "    \n",
    "    def Summarize(self,url):\n",
    "        import pandas as pd\n",
    "        import datetime\n",
    "        #import csv\n",
    "        import numpy as np\n",
    "        import time\n",
    "        starttime=time.time()\n",
    "        exclude=['twitter','glassdoor','youtube','wordpress','facebook','wikipedia','play.google','nasdaq']\n",
    "        startTime_Main = datetime.datetime.now().strftime('%H:%M:%S')\n",
    "        summarylist=[]\n",
    "        summary1=\"\"\n",
    "        #print(\"---START TIME %s ---\" % startTime_Main)\n",
    "        for url in urls:\n",
    "            if any (s in url for s in exclude):\n",
    "                #print(\"Not allowed -\",url)\n",
    "                summary1=\"Not Allowed {}\".format(url)\n",
    "            else:\n",
    "                try:\n",
    "                    article=self.GetText(url)\n",
    "                except BaseException as e:\n",
    "                    article=\"ERROR {} {}\".format(e,url)\n",
    "\n",
    "                if not article.startswith(\"ERROR\"):\n",
    "                    print(\"fetching summary for - \",url)\n",
    "                    summary1=self.Summary(article)\n",
    "                else:\n",
    "                    summary1=article\n",
    "\n",
    "            summaryDict[url]=summary1\n",
    "        #print(\"{:.2f}\".format((time.time()-starttime)))\n",
    "        return summary1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarize=Extract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'First major deal completed by government since collapse of Carillion condemned by unions  Angela Monaghan   Tue 19 Jun 2018 20.24\\xa0BST   Last modified on Wed 20 Jun 2018 00.05\\xa0BST   Capita has been awarded a new contract from the Ministry of Defence to run UK military fire and rescue services despite a financial health assessment that attached the highest possible risk rating to the outsourcing company. The awarding of the contract was condemned by unions. It is the first major deal completed by the government since the collapse of the rival firm Carillion in January, which put the outsourcing of the public sector under the spotlight.  Capita scored 10 out of 10 for risk in a document prepared for the MoD. One indicates the lowest probability of distress and 10 the highest. The risk assessment document was updated on 7 June, according to the Financial Times. On Tuesday Capita confirmed it had been selected for the contract, which is estimated to be worth about £500m over 10 years and involves the running of about 70 military fire stations worldwide. Shares in the company, which also announced it had sold its supply chain business Supplier Assessment Services, raising £160m, according to analysts, closed up nearly 8% at 164p, before the FT first reported the 10/10 risk assessment score. As part of the risk assessment Capita was also awarded a “health score” of just three out of 100. A score of 25 or less is deemed to be in a red “warning area” of heightened vulnerability. Capita beat Serco to the MoD deal, with the FT reporting that the rival bidder was judged an eight out of 10 risk. The MoD said Capita’s bid was considered to be the best technical solution and the best value for money for defence. A spokesperson said: “All our suppliers are subject to robust assessments ahead of any contract placement and closely monitored throughout.  “This document provides background information, using a range of statistics and figures from external sources. The ratings in question are from [financial consultants] Company Watch, not the MoD.” Unite condemned the decision to award the contract to Capita. Jim Kennedy, the union’s national officer for MoD workers, said: “It is absolutely scandalous that even though the MoD’s own financial check found that awarding a contract to Capita was extremely risky, ministers thought it appropriate to award this contract. “The government has clearly learnt nothing from the Carillion fiasco. There needs to be an urgent investigation into how and why this contract was let to Capita.”  Capita came under the spotlight just days after the extent of Carillion’s woes came to light, when the new chief executive, Jon Lewis, announced a plans to tap the market for £700m of investment and suspended a dividend that was worth more than £200m to shareholders last year.'"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarize.GetText(\"https://www.theguardian.com/business/2018/jun/19/capita-awarded-mod-contract-despite-having-highest-risk-rating\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---START TIME 16:00:11 ---\n",
      "fetching summary for -  https://www.theguardian.com/business/2018/jun/19/capita-awarded-mod-contract-despite-having-highest-risk-rating\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'First major deal completed by government since collapse of Carillion condemned by unions  Angela Monaghan   Tue 19 Jun 2018 20.24\\xa0BST   Last modified on Wed 20 Jun 2018 00.05\\xa0BST   Capita has been awarded a new contract from the Ministry of Defence to run UK military fire and rescue services despite a financial health assessment that attached the highest possible risk rating to the outsourcing company. It is the first major deal completed by the government since the collapse of the rival firm Carillion in January, which put the outsourcing of the public sector under the spotlight. There needs to be an urgent investigation into how and why this contract was let to Capita.”  Capita came under the spotlight just days after the extent of Carillion’s woes came to light, when the new chief executive, Jon Lewis, announced a plans to tap the market for £700m of investment and suspended a dividend that was worth more than £200m to shareholders last year.'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarize.Summarize(\"https://www.theguardian.com/business/2018/jun/19/capita-awarded-mod-contract-despite-having-highest-risk-rating\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.dailymail.co.uk/sciencetech/article-6144487/Scientists-release-highest-resolution-map-Antarctica-made.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"./data/ALL.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.acticles, data.category, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.DataFrame({\"Arcticles\":X_test,\"Category\":y_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(223, 15235)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(X_train)\n",
    "X_train_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(223, 15235)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tf_transformer = TfidfTransformer(use_idf=False).fit(X_train_counts)\n",
    "X_train_tf = tf_transformer.transform(X_train_counts)\n",
    "X_train_tf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(223, 15235)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "newSum=ExtractNew()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=summarize.GetText(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "url=[\"https://www.dailymail.co.uk/sport/football/article-6117823/Messi-Fabregas-Pique-happened-rest-Barcelonas-Class-87.html\"\n",
    ",\"https://www.dailymail.co.uk/sciencetech/article-6144487/Scientists-release-highest-resolution-map-Antarctica-made.html\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "text_clf = Pipeline([(\"pre\",newSum),\n",
    "                     ('vect', CountVectorizer()),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', SGDClassifier(loss='hinge', penalty='l2',\n",
    "                                           alpha=1e-3, random_state=42,\n",
    "                                           max_iter=5, tol=None)),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('pre', ExtractNew()), ('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_wo...ty='l2', power_t=0.5, random_state=42, shuffle=True,\n",
       "       tol=None, verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_clf.fit(X=url,y=[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = text_clf.predict([\"https://www.dailymail.co.uk/sport/football/article-6150725/Mourinho-Klopp-Guardiola-managers-fared-early-stages-season.html\"])\n",
    "#np.mean(predicted == y_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "     Sports       0.96      0.96      0.96        26\n",
      "    Science       0.90      0.95      0.93        59\n",
      "     Femail       0.83      0.73      0.78        26\n",
      "\n",
      "avg / total       0.90      0.90      0.90       111\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print(metrics.classification_report(y_test, predicted,target_names=[\"Sports\",\"Science\",\"Femail\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[25,  0,  1],\n",
       "       [ 0, 56,  3],\n",
       "       [ 1,  6, 19]], dtype=int64)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.confusion_matrix(y_test, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
